{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import MeCab\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from bs4 import BeautifulSoup\n",
    "import MySQLdb\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connector_and_cursor():\n",
    "    conn = MySQLdb.connect(\n",
    "        host = '0.0.0.0',\n",
    "        port = 3306,\n",
    "        user = 'root',\n",
    "        password = 'root',\n",
    "        database = 'maindb'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ポイント予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_number(mecab, text):\n",
    "    text = str(text)\n",
    "    count = []\n",
    "    for line in mecab.parse(text).splitlines():\n",
    "        try:\n",
    "            if \"名詞\" in line.split()[-1]:\n",
    "                count.append(line)\n",
    "        except:\n",
    "            pass\n",
    "    return len(set(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(detail_df):\n",
    "    '''\n",
    "    Made features: \n",
    "        title_length: length of title\n",
    "        story_length: length of story\n",
    "        text_length: length of text\n",
    "        keyword_number: number of keywords\n",
    "        noun_proportion_in_text: number of nouns in text per text length\n",
    "    '''\n",
    "    mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    for column in ['title', 'story', 'text']:\n",
    "        detail_df[column + '_length'] = detail_df[column].apply(lambda x: len(str(x)))\n",
    "    detail_df['keyword_number'] = detail_df['keyword'].apply(lambda x: len(str(x).split(' ')))\n",
    "    detail_df['noun_proportion_in_text'] = detail_df.text.apply(lambda x: count_noun_number(mecab, str(x)) / len(str(x)))\n",
    "    return detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_prediction(url, detail_df):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of point prediction api\n",
    "        pandas.DataFrame detail_df: dataframe containing all features of item\n",
    "    '''\n",
    "    detail_df = preprocessing(detail_df)\n",
    "    \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {}\n",
    "    data = {column: list(detail_df[column]) for column in list(detail_df.columns)}\n",
    "    data = json.dumps(data)\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    predicted_points = r_post.json()['prediction']\n",
    "    return predicted_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of feature extraction api\n",
    "        list<str> texts: texts of narou novel\n",
    "    Return:\n",
    "        list<float> features: feature vectors of item\n",
    "    '''   \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {'texts': texts}\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "    features = r_post.json()['prediction']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(ncodes, features):\n",
    "    for ncode, feature in zip(ncodes, features):\n",
    "        yield {\n",
    "            '_index': 'features',\n",
    "            'ncode': ncode,\n",
    "            'feature': feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_features_to_elasticsearch(host, url, ncodes, texts, h_dim=64):\n",
    "    '''\n",
    "    Args: \n",
    "        str host: host name of elasticsearch\n",
    "        str url: url of feature extraction api\n",
    "        list<str> ncodes: ncodes to register\n",
    "        texts<str> texts: texts to extract features\n",
    "        h_dim: size of feature vector\n",
    "    '''    \n",
    "\n",
    "    features = extract_features(url, texts)\n",
    "    \n",
    "    client = Elasticsearch(host)\n",
    "    \n",
    "    mappings = {\n",
    "        'properties': {\n",
    "            'ncode': {'type': 'text'},\n",
    "            'feature': {'type': 'dense_vector', 'dims': h_dim}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not client.indices.exists(index='features'):\n",
    "        client.indices.create(index='features', body={ 'mappings': mappings })\n",
    "    \n",
    "    bulk(client, generate_data(ncodes, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全データポイント予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "MINIBATCH_SIZE = 10\n",
    "ELASTICSEARCH_HOST_NAME = 'localhost:9200'\n",
    "FEATURE_EXTRACTION_URL = 'http://localhost:3032/predict'\n",
    "POINT_PREDICTION_URL = 'http://localhost:3033/predict'\n",
    "\n",
    "def register_all_data():\n",
    "    conn, cursor = get_connector_and_cursor()\n",
    "    detail_df = pd.read_sql_query(\"SELECT * FROM details WHERE predict_point='Nan'\", conn)\n",
    "    predicted_point = point_prediction(POINT_PREDICTION_URL, detail_df)\n",
    "    detail_df['predict_point'] = predicted_point\n",
    "    target_detail_df = detail_df[(detail_df['predict_point']==1) & (detail_df['global_point']==0)]\n",
    "    if len(target_detail_df) != 0:      \n",
    "        ncodes = list(target_detail_df.ncode)\n",
    "        texts = list(target_detail_df.text)\n",
    "\n",
    "        for i in range(len(ncodes) // MINIBATCH_SIZE + 1):\n",
    "            register_features_to_elasticsearch(ELASTICSEARCH_HOST_NAME, FEATURE_EXTRACTION_URL, ncodes[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE], texts[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE])\n",
    "            if TEST:\n",
    "                break\n",
    "\n",
    "    print('{} data is inserted to Elasticsearch.'.format(len(target_detail_df)))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preprocessing(df):\n",
    "    df = df.drop(['allcount', 'gensaku'], axis=1, errors='ignore')\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    date_to_timestamp = lambda date: int(datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column in ['title', 'ncode', 'userid', 'writer', 'story', 'keyword']:\n",
    "            df[column] = df[column].astype(str)\n",
    "        elif column in['general_firstup', 'general_lastup', 'novelupdated_at', 'updated_at']:\n",
    "            df[column] = df[column].map(str).map(date_to_timestamp)\n",
    "        else:\n",
    "            df[column] = df[column].astype(int)\n",
    "            \n",
    "    df['predict_point'] = 'Nan'\n",
    "    df['text'] = 'Nan'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_details(conn, cursor, narou_api_url, mode='middle', test=True):\n",
    "    if mode not in ['middle', 'first']:\n",
    "        raise Exception('Argument mode should be middle or first.')\n",
    "    \n",
    "    if mode is 'middle':\n",
    "        cursor.execute('SELECT general_lastup FROM details ORDER BY general_lastup DESC LIMIT 1')\n",
    "        sql_result = cursor.fetchone()\n",
    "        register_latest = str(sql_result[0]) if sql_result is not None else \"1073779200\"\n",
    "    elif mode is 'first':\n",
    "        register_latest = \"1073779200\"\n",
    "    \n",
    "    now = str(int(datetime.datetime.now().timestamp()))\n",
    "\n",
    "    payload = {'out': 'json', 'gzip': 5, 'of': 'n', 'lim': 1, 'lastup': register_latest+\"-\"+now}\n",
    "    res = requests.get(narou_api_url, params=payload).content\n",
    "    r =  gzip.decompress(res).decode(\"utf-8\") \n",
    "    allcount = json.loads(r)[0][\"allcount\"]\n",
    "    \n",
    "    interval = 1\n",
    "    detail_df = pd.DataFrame()\n",
    "\n",
    "    lastup = now\n",
    "    all_queue_cnt = (allcount // 500)\n",
    "\n",
    "    for i in range(all_queue_cnt):\n",
    "        payload = {'out': 'json', 'gzip': 5,'opt': 'weekly', 'lim':500, 'lastup': register_latest+\"-\"+str(lastup)}\n",
    "        \n",
    "        c = 0 # Avoid infinite loop\n",
    "        while c < 10:\n",
    "            try:\n",
    "                res = requests.get(narou_api_url, params=payload, timeout=30).content\n",
    "                break\n",
    "            except:\n",
    "                print('Connection Error')\n",
    "                c += 1       \n",
    "\n",
    "        r = gzip.decompress(res).decode('utf-8')\n",
    "\n",
    "        df_temp = pd.read_json(r)\n",
    "        df_temp = df_temp.drop(0)\n",
    "\n",
    "        last_general_lastup = df_temp.iloc[-1][\"general_lastup\"]\n",
    "        lastup = datetime.datetime.strptime(last_general_lastup, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "        lastup = int(lastup)\n",
    "\n",
    "        df_temp = df_preprocessing(df_temp)\n",
    "        detail_df = pd.concat([detail_df, df_temp], axis=0)\n",
    "\n",
    "        time.sleep(interval)\n",
    "        \n",
    "        if test is True and i==1:\n",
    "            break\n",
    "        \n",
    "    return detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bs_obj(url):\n",
    "    html = urlopen(url)\n",
    "    return BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_text(bs_obj):\n",
    "    text = \"\"\n",
    "    text_htmls = bs_obj.findAll(\"div\",{\"id\":\"novel_honbun\"})[0].findAll(\"p\")\n",
    "\n",
    "    for text_html in text_htmls:\n",
    "        text = text + text_html.get_text() + \"\\n\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_texts(ncodes, test=True):\n",
    "    texts = []\n",
    "    processed_ncodes = []\n",
    "    interval = 0.1\n",
    "    cnt = 0\n",
    "\n",
    "    for ncode in ncodes:\n",
    "        print(cnt) if cnt % 100 == 0 else None\n",
    "\n",
    "        time.sleep(interval)\n",
    "        url = 'https://ncode.syosetu.com/' + ncode + '/'\n",
    "        c = 0 # Avoid infinite loop\n",
    "        while c < 10:\n",
    "            try:\n",
    "                bs_obj = make_bs_obj(url)\n",
    "                break\n",
    "            except:\n",
    "                print('Connection Error')\n",
    "                c += 1\n",
    "                \n",
    "        url_list = [\"https://ncode.syosetu.com\" + a_bs_obj.find(\"a\").attrs[\"href\"] for a_bs_obj in bs_obj.findAll(\"dl\", {\"class\": \"novel_sublist2\"})]\n",
    "        \n",
    "        if len(url_list) == 0:\n",
    "            text = get_main_text(bs_obj)\n",
    "        else:\n",
    "            time.sleep(interval)\n",
    "            bs_obj = make_bs_obj(url_list[0])\n",
    "            text = get_main_text(bs_obj)\n",
    "\n",
    "        texts.append(text)\n",
    "        processed_ncodes.append(ncode)\n",
    "        cnt += 1\n",
    "        \n",
    "        if test == True and cnt == 10:\n",
    "            break\n",
    "    \n",
    "    return processed_ncodes, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "MINIBATCH_SIZE = 10\n",
    "NAROU_API_URL = 'https://api.syosetu.com/novelapi/api/'\n",
    "ELASTICSEARCH_HOST_NAME = 'localhost:9200'\n",
    "FEATURE_EXTRACTION_URL = 'http://localhost:3032/predict'\n",
    "POINT_PREDICTION_URL = 'http://localhost:3033/predict'\n",
    "\n",
    "def register_scraped_data():\n",
    "    conn, cursor = get_connector_and_cursor()\n",
    "    # Scraping details and texts\n",
    "    detail_df = scraping_details(conn, cursor, NAROU_API_URL, mode='first', test=TEST)\n",
    "    ncodes, texts = scraping_texts(detail_df.ncode, TEST)\n",
    "    for ncode, text in zip(ncodes, texts):\n",
    "        detail_df.loc[detail_df['ncode'] == ncode, 'text'] = text\n",
    "    predicted_point = point_prediction(POINT_PREDICTION_URL, detail_df)\n",
    "    detail_df['predict_point'] = predicted_point\n",
    "    \n",
    "    # Insert scraped data to database\n",
    "    cursor.execute('SHOW columns FROM details')\n",
    "    columns_of_details = [column[0] for column in cursor.fetchall()]\n",
    "    details_data_tmp = detail_df[columns_of_details]\n",
    "    details_data = [tuple(details_data_tmp.iloc[i]) for i in range(len(details_data_tmp))]\n",
    "    cursor.executemany(\"INSERT INTO details VALUES ({})\".format(('%s, '*len(columns_of_details))[:-2]), details_data)\n",
    "    \n",
    "    # Insert scraped data to elasticsearch\n",
    "    target_detail_df = detail_df[(detail_df['predict_point'] == 1) & (detail_df['global_point'] == 0)]\n",
    "    if len(target_detail_df) != 0:      \n",
    "        ncodes = list(target_detail_df.ncode)\n",
    "        texts = list(target_detail_df.text)\n",
    "\n",
    "        for i in range(len(ncodes) // MINIBATCH_SIZE + 1):\n",
    "            register_features_to_elasticsearch(ELASTICSEARCH_HOST_NAME, FEATURE_EXTRACTION_URL, ncodes[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE], texts[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE])\n",
    "            if TEST:\n",
    "                break\n",
    "    \n",
    "    print('{} data is inserted to Elasticsearch.'.format(len(target_detail_df)))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "76 data is inserted to Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "register_scraped_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ncode_index(cursor):\n",
    "    cursor.execute(\"CREATE INDEX ncodeindex ON details(ncode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ポイント予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_number(mecab, text):\n",
    "    text = str(text)\n",
    "    count = []\n",
    "    for line in mecab.parse(text).splitlines():\n",
    "        try:\n",
    "            if \"名詞\" in line.split()[-1]:\n",
    "                count.append(line)\n",
    "        except:\n",
    "            pass\n",
    "    return len(set(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(detail_df):\n",
    "    '''\n",
    "    Made features: \n",
    "        title_length: length of title\n",
    "        story_length: length of story\n",
    "        text_length: length of text\n",
    "        keyword_number: number of keywords\n",
    "        noun_proportion_in_text: number of nouns in text per text length\n",
    "    '''\n",
    "    mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    for column in ['title', 'story', 'text']:\n",
    "        detail_df[column + '_length'] = detail_df[column].apply(lambda x: len(str(x)))\n",
    "    detail_df['keyword_number'] = detail_df['keyword'].apply(lambda x: len(str(x).split(' ')))\n",
    "    detail_df['noun_proportion_in_text'] = detail_df.text.apply(lambda x: count_noun_number(mecab, str(x)) / len(str(x)))\n",
    "    return detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_prediction(url, detail_df):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of point prediction api\n",
    "        pandas.DataFrame detail_df: dataframe containing all features of item\n",
    "    '''\n",
    "    detail_df = preprocessing(detail_df)\n",
    "    \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {}\n",
    "    data = {column: list(detail_df[column]) for column in list(detail_df.columns)}\n",
    "    data = json.dumps(data)\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    predicted_points = r_post.json()['prediction']\n",
    "    return predicted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_predicted_point(url, conn, cursor, mode, ncodes=None, test=True):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of point prediction api\n",
    "        sqlite3.Connection conn: connection of sqlite3\n",
    "        sqlite3.Cursor cursor: cursor of sqlite3\n",
    "        str mode: all (target all records whose predict_point is Nan) or part (specify records by ncode)\n",
    "        list<str> ncodes: specify ncode when use part mode\n",
    "    '''   \n",
    "    \n",
    "    if mode not in ['all', 'part']:\n",
    "        raise Exception('Argument mode shoud be all or part.')\n",
    "    if mode == 'part' and type(ncodes) is not list:\n",
    "        raise Exception('Argument ncodes should be list of string.')\n",
    "    \n",
    "    if mode == 'all':\n",
    "        if test == True:\n",
    "            detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE predict_point='Nan' LIMIT 10\", con=conn)\n",
    "        else:\n",
    "            detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE predict_point='Nan'\", con=conn)\n",
    "        ncodes = list(detail_df.ncode)\n",
    "    elif mode == 'part':\n",
    "        ncodes_str = ', '.join(map(str, list([\"'{}'\".format(ncode) for ncode in ncodes])))\n",
    "        detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE ncode IN ({})\".format(ncodes_str), con=conn)\n",
    "\n",
    "    predicted_points = point_prediction(url, detail_df)\n",
    "    \n",
    "    ncodes = list(detail_df.ncode)\n",
    "    for ncode, predicted_point in zip(ncodes, predicted_points):\n",
    "        c.execute(\"UPDATE details SET predict_point={} WHERE ncode='{}'\".format(predicted_point, ncode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://localhost:5000/predict'\n",
    "url = 'http://localhost:3033/predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2292ed06103e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregister_predicted_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-cc6e4388563b>\u001b[0m in \u001b[0;36mregister_predicted_point\u001b[0;34m(url, conn, cursor, mode, ncodes, test)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdetail_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SELECT * FROM details WHERE ncode IN ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncodes_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpredicted_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mncodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2819e244dd2b>\u001b[0m in \u001b[0;36mpoint_prediction\u001b[0;34m(url, detail_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mr_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpredicted_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "register_predicted_point(url, conn, cursor, mode='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of feature extraction api\n",
    "        list<str> texts: texts of narou novel\n",
    "    Return:\n",
    "        list<float> features: feature vectors of item\n",
    "    '''   \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {'texts': texts}\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "    features = r_post.json()['prediction']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(ncodes, features):\n",
    "    for ncode, feature in zip(ncodes, features):\n",
    "        yield {\n",
    "            '_index': 'features',\n",
    "            'ncode': ncode,\n",
    "            'feature': feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_features_to_elasticsearch(host, url, ncodes, texts, h_dim=64):\n",
    "    '''\n",
    "    Args: \n",
    "        str host: host name of elasticsearch\n",
    "        str url: url of feature extraction api\n",
    "        list<str> ncodes: ncodes to register\n",
    "        texts<str> texts: texts to extract features\n",
    "        h_dim: size of feature vector\n",
    "    '''    \n",
    "\n",
    "    features = extract_features(url, texts)\n",
    "    \n",
    "    client = Elasticsearch(host)\n",
    "    \n",
    "    mappings = {\n",
    "        'properties': {\n",
    "            'ncode': {'type': 'text'},\n",
    "            'feature': {'type': 'dense_vector', 'dims': h_dim}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not client.indices.exists(index='features'):\n",
    "        client.indices.create(index='features', body={ 'mappings': mappings })\n",
    "    \n",
    "    bulk(client, generate_data(ncodes, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_all_features_to_elasticsearch(conn, host, url, h_dim=64, test=True):\n",
    "    \n",
    "    if test == True:\n",
    "        detail_df = pd.read_sql_query(sql=\"SELECT * FROM details LIMIT 30\", con=conn)\n",
    "    else:\n",
    "        detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE predict_point=1\", con=conn)\n",
    "    \n",
    "    ncodes = list(detail_df.ncode)\n",
    "    texts = list(detail_df.text)\n",
    "    \n",
    "    register_features_to_elasticsearch(host, url, ncodes, texts, h_dim=h_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost:9200'\n",
    "# url = 'http://localhost:5000/predict'\n",
    "url = 'http://localhost:3032/predict'\n",
    "h_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "register_all_features_to_elasticsearch(conn, host, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 類似文書検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_text(url, query_text):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of feature extraction api\n",
    "        str query_text: query of similar text search\n",
    "    '''\n",
    "    if type(query_text) is not list and type(query_text) is not str:\n",
    "        raise Exception('query_text should be list or str.')\n",
    "    if type(query_text) is str:\n",
    "        query_text = [query_text]\n",
    "        \n",
    "    query_feature = extract_features(url, query_text)[0]\n",
    "    \n",
    "    res = es.search(index='features', body={\n",
    "      \"query\": {\n",
    "        \"script_score\": {\n",
    "          \"query\": {\n",
    "            \"match_all\": {}\n",
    "          },\n",
    "          \"script\": {\n",
    "            \"source\": \"cosineSimilarity(params.query_vec, doc['feature']) + 1.0\", # Elasticsearch does not allow negative scores\n",
    "            \"params\": {\n",
    "              \"query_vec\": query_feature\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    \n",
    "    recommend_ncodes = []\n",
    "    for i in range(10):\n",
    "        ncode = res['hits']['hits'][i]['_source']['ncode']\n",
    "        recommend_ncodes.append(ncode)\n",
    "    \n",
    "    return recommend_ncodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N6600GI',\n",
       " 'N2238GK',\n",
       " 'N3325BS',\n",
       " 'N9514FJ',\n",
       " 'N1762DR',\n",
       " 'N8231FM',\n",
       " 'N9831GA',\n",
       " 'N1703GK',\n",
       " 'N0137GI',\n",
       " 'N2241GK']"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_similar_text(url, 'testtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
