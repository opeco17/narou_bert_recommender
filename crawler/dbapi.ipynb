{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlite3\n",
    "import MeCab\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from bs4 import BeautifulSoup\n",
    "import MySQLdb\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MySQL接続"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connector_and_cursor():\n",
    "    conn = MySQLdb.connect(\n",
    "        host = '0.0.0.0',\n",
    "        port = 3306,\n",
    "        user = 'root',\n",
    "        password = 'root',\n",
    "        database = 'maindb'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ポイント予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_number(mecab, text):\n",
    "    text = str(text)\n",
    "    count = []\n",
    "    for line in mecab.parse(text).splitlines():\n",
    "        try:\n",
    "            if \"名詞\" in line.split()[-1]:\n",
    "                count.append(line)\n",
    "        except:\n",
    "            pass\n",
    "    return len(set(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(detail_df):\n",
    "    '''\n",
    "    New features: \n",
    "        title_length: length of title\n",
    "        story_length: length of story\n",
    "        text_length: length of text\n",
    "        keyword_number: number of keywords\n",
    "        noun_proportion_in_text: number of nouns in text per text length\n",
    "    '''\n",
    "    mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    for column in ['title', 'story', 'text']:\n",
    "        detail_df[column + '_length'] = detail_df[column].apply(lambda x: len(str(x)))\n",
    "    detail_df['keyword_number'] = detail_df['keyword'].apply(lambda x: len(str(x).split(' ')))\n",
    "    detail_df['noun_proportion_in_text'] = detail_df.text.apply(lambda x: count_noun_number(mecab, str(x)) / len(str(x)))\n",
    "    return detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_prediction(url, detail_df):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of point prediction api\n",
    "        pandas.DataFrame detail_df: dataframe containing all features of item\n",
    "    '''\n",
    "    detail_df = preprocessing(detail_df)\n",
    "    \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {}\n",
    "    data = {column: list(detail_df[column]) for column in list(detail_df.columns)}\n",
    "    data = json.dumps(data)\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    predicted_points = r_post.json()['prediction']\n",
    "    return predicted_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of feature extraction api\n",
    "        list<str> texts: texts of narou novel\n",
    "    Return:\n",
    "        list<float> features: feature vectors of item\n",
    "    '''   \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {'texts': texts}\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "    features = r_post.json()['prediction']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(ncodes, features):\n",
    "    for ncode, feature in zip(ncodes, features):\n",
    "        yield {\n",
    "            '_index': 'features',\n",
    "            'ncode': ncode,\n",
    "            'feature': feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_features_to_elasticsearch(host, url, ncodes, texts, h_dim=64):\n",
    "    '''\n",
    "    Args: \n",
    "        str host: host name of elasticsearch\n",
    "        str url: url of feature extraction api\n",
    "        list<str> ncodes: ncodes to register\n",
    "        texts<str> texts: texts to extract features\n",
    "        h_dim: size of feature vector\n",
    "    '''    \n",
    "\n",
    "    features = extract_features(url, texts)\n",
    "    \n",
    "    client = Elasticsearch(host)\n",
    "    \n",
    "    mappings = {\n",
    "        'properties': {\n",
    "            'ncode': {'type': 'keyword'},\n",
    "            'feature': {'type': 'dense_vector', 'dims': h_dim}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not client.indices.exists(index='features'):\n",
    "        client.indices.create(index='features', body={ 'mappings': mappings })\n",
    "    \n",
    "    bulk(client, generate_data(ncodes, features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全データポイント予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "MINIBATCH_SIZE = 10\n",
    "ELASTICSEARCH_HOST_NAME = 'localhost:9200'\n",
    "FEATURE_EXTRACTION_URL = 'http://localhost:3032/predict'\n",
    "POINT_PREDICTION_URL = 'http://localhost:3033/predict'\n",
    "\n",
    "def register_all_data():\n",
    "    conn, cursor = get_connector_and_cursor()\n",
    "#     detail_df = pd.read_sql_query(\"SELECT * FROM details WHERE predict_point='Nan'\", conn)\n",
    "    detail_df = pd.read_sql_query(\"SELECT * FROM details LIMIT 10\", conn)\n",
    "    print(detail_df)\n",
    "    predicted_point = point_prediction(POINT_PREDICTION_URL, detail_df)\n",
    "    detail_df['predict_point'] = predicted_point\n",
    "    target_detail_df = detail_df[(detail_df['predict_point']==1) & (detail_df['global_point']==0)]\n",
    "    if len(target_detail_df) != 0:      \n",
    "        ncodes = list(target_detail_df.ncode)\n",
    "        texts = list(target_detail_df.text)\n",
    "\n",
    "        for i in range(len(ncodes) // MINIBATCH_SIZE + 1):\n",
    "            register_features_to_elasticsearch(ELASTICSEARCH_HOST_NAME, FEATURE_EXTRACTION_URL, ncodes[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE], texts[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE])\n",
    "            if TEST:\n",
    "                break\n",
    "\n",
    "    print('{} data is inserted to Elasticsearch.'.format(len(target_detail_df)))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [title, ncode, userid, writer, story, biggenre, genre, keyword, general_firstup, general_lastup, novel_type, end, general_all_no, length, time, isstop, isr15, isbl, isgl, iszankoku, istensei, istenni, pc_or_k, global_point, daily_point, weekly_point, monthly_point, quarter_point, yearly_point, fav_novel_cnt, impression_cnt, review_cnt, all_point, all_hyoka_cnt, sasie_cnt, kaiwaritu, novelupdated_at, updated_at, weekly_unique, text, predict_point]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-6facc3def1ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregister_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-103-531ed836d6e5>\u001b[0m in \u001b[0;36mregister_all_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdetail_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM details LIMIT 10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpredicted_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPOINT_PREDICTION_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdetail_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_point'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtarget_detail_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetail_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict_point'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdetail_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'global_point'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-93-2819e244dd2b>\u001b[0m in \u001b[0;36mpoint_prediction\u001b[0;34m(url, detail_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mr_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpredicted_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "register_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preprocessing(df):\n",
    "    df = df.drop(['allcount', 'gensaku'], axis=1, errors='ignore')\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    date_to_timestamp = lambda date: int(datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column in ['title', 'ncode', 'userid', 'writer', 'story', 'keyword']:\n",
    "            df[column] = df[column].astype(str)\n",
    "        elif column in['general_firstup', 'general_lastup', 'novelupdated_at', 'updated_at']:\n",
    "            df[column] = df[column].map(str).map(date_to_timestamp)\n",
    "        else:\n",
    "            df[column] = df[column].astype(int)\n",
    "            \n",
    "    df['predict_point'] = 'Nan'\n",
    "    df['text'] = 'Nan'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_details(conn, cursor, narou_api_url, mode='middle', test=True):\n",
    "    if mode not in ['middle', 'first']:\n",
    "        raise Exception('Argument mode should be middle or first.')\n",
    "    \n",
    "    if mode is 'middle':\n",
    "        cursor.execute('SELECT general_lastup FROM details ORDER BY general_lastup DESC LIMIT 1')\n",
    "        sql_result = cursor.fetchone()\n",
    "        register_latest = str(sql_result[0]) if sql_result is not None else \"1073779200\"\n",
    "    elif mode is 'first':\n",
    "        register_latest = \"1073779200\"\n",
    "    \n",
    "    now = str(int(datetime.datetime.now().timestamp()))\n",
    "\n",
    "    payload = {'out': 'json', 'gzip': 5, 'of': 'n', 'lim': 1, 'lastup': register_latest+\"-\"+now}\n",
    "    res = requests.get(narou_api_url, params=payload).content\n",
    "    r =  gzip.decompress(res).decode(\"utf-8\") \n",
    "    allcount = json.loads(r)[0][\"allcount\"]\n",
    "    \n",
    "    interval = 1\n",
    "    detail_df = pd.DataFrame()\n",
    "\n",
    "    lastup = now\n",
    "    all_queue_cnt = (allcount // 500)\n",
    "\n",
    "    for i in range(all_queue_cnt):\n",
    "        payload = {'out': 'json', 'gzip': 5,'opt': 'weekly', 'lim':500, 'lastup': register_latest+\"-\"+str(lastup)}\n",
    "        \n",
    "        c = 0 # Avoid infinite loop\n",
    "        while c < 10:\n",
    "            try:\n",
    "                res = requests.get(narou_api_url, params=payload, timeout=30).content\n",
    "                break\n",
    "            except:\n",
    "                print('Connection Error')\n",
    "                c += 1       \n",
    "\n",
    "        r = gzip.decompress(res).decode('utf-8')\n",
    "\n",
    "        df_temp = pd.read_json(r)\n",
    "        df_temp = df_temp.drop(0)\n",
    "\n",
    "        last_general_lastup = df_temp.iloc[-1][\"general_lastup\"]\n",
    "        lastup = datetime.datetime.strptime(last_general_lastup, \"%Y-%m-%d %H:%M:%S\").timestamp()\n",
    "        lastup = int(lastup)\n",
    "\n",
    "        df_temp = df_preprocessing(df_temp)\n",
    "        detail_df = pd.concat([detail_df, df_temp], axis=0)\n",
    "\n",
    "        time.sleep(interval)\n",
    "        \n",
    "        if test is True and i==1:\n",
    "            break\n",
    "        \n",
    "    return detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bs_obj(url):\n",
    "    html = urlopen(url)\n",
    "    return BeautifulSoup(html,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_text(bs_obj):\n",
    "    text = \"\"\n",
    "    text_htmls = bs_obj.findAll(\"div\",{\"id\":\"novel_honbun\"})[0].findAll(\"p\")\n",
    "\n",
    "    for text_html in text_htmls:\n",
    "        text = text + text_html.get_text() + \"\\n\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_texts(ncodes, test=True):\n",
    "    texts = []\n",
    "    processed_ncodes = []\n",
    "    interval = 0.1\n",
    "    cnt = 0\n",
    "\n",
    "    for ncode in ncodes:\n",
    "        print(cnt) if cnt % 100 == 0 else None\n",
    "\n",
    "        time.sleep(interval)\n",
    "        url = 'https://ncode.syosetu.com/' + ncode + '/'\n",
    "        c = 0 # Avoid infinite loop\n",
    "        while c < 10:\n",
    "            try:\n",
    "                bs_obj = make_bs_obj(url)\n",
    "                break\n",
    "            except:\n",
    "                print('Connection Error')\n",
    "                c += 1\n",
    "                \n",
    "        url_list = [\"https://ncode.syosetu.com\" + a_bs_obj.find(\"a\").attrs[\"href\"] for a_bs_obj in bs_obj.findAll(\"dl\", {\"class\": \"novel_sublist2\"})]\n",
    "        \n",
    "        if len(url_list) == 0:\n",
    "            text = get_main_text(bs_obj)\n",
    "        else:\n",
    "            time.sleep(interval)\n",
    "            bs_obj = make_bs_obj(url_list[0])\n",
    "            text = get_main_text(bs_obj)\n",
    "\n",
    "        texts.append(text)\n",
    "        processed_ncodes.append(ncode)\n",
    "        cnt += 1\n",
    "        \n",
    "        if test == True and cnt == 10:\n",
    "            break\n",
    "    \n",
    "    return processed_ncodes, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "MINIBATCH_SIZE = 10\n",
    "NAROU_API_URL = 'https://api.syosetu.com/novelapi/api/'\n",
    "ELASTICSEARCH_HOST_NAME = 'localhost:9200'\n",
    "FEATURE_EXTRACTION_URL = 'http://localhost:3032/predict'\n",
    "POINT_PREDICTION_URL = 'http://localhost:3033/predict'\n",
    "\n",
    "def register_scraped_data():\n",
    "    conn, cursor = get_connector_and_cursor()\n",
    "    # Scraping details and texts\n",
    "    detail_df = scraping_details(conn, cursor, NAROU_API_URL, mode='first', test=TEST)\n",
    "    ncodes, texts = scraping_texts(detail_df.ncode, TEST)\n",
    "    for ncode, text in zip(ncodes, texts):\n",
    "        detail_df.loc[detail_df['ncode'] == ncode, 'text'] = text\n",
    "    predicted_point = point_prediction(POINT_PREDICTION_URL, detail_df)\n",
    "    detail_df['predict_point'] = predicted_point\n",
    "    \n",
    "    # Insert scraped data to database\n",
    "    cursor.execute('SHOW columns FROM details')\n",
    "    columns_of_details = [column[0] for column in cursor.fetchall()]\n",
    "    details_data_tmp = detail_df[columns_of_details]\n",
    "    details_data = [tuple(details_data_tmp.iloc[i]) for i in range(len(details_data_tmp))]\n",
    "    cursor.executemany(\"INSERT INTO details VALUES ({})\".format(('%s, '*len(columns_of_details))[:-2]), details_data)\n",
    "    \n",
    "    # Insert scraped data to elasticsearch\n",
    "    target_detail_df = detail_df[(detail_df['predict_point'] == 1) & (detail_df['global_point'] == 0)]\n",
    "    if len(target_detail_df) != 0:      \n",
    "        ncodes = list(target_detail_df.ncode)\n",
    "        texts = list(target_detail_df.text)\n",
    "\n",
    "        for i in range(len(ncodes) // MINIBATCH_SIZE + 1):\n",
    "            register_features_to_elasticsearch(ELASTICSEARCH_HOST_NAME, FEATURE_EXTRACTION_URL, ncodes[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE], texts[i*MINIBATCH_SIZE:(i+1)*MINIBATCH_SIZE])\n",
    "            if TEST:\n",
    "                break\n",
    "    \n",
    "    print('{} data is inserted to Elasticsearch.'.format(len(target_detail_df)))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "76 data is inserted to Elasticsearch.\n"
     ]
    }
   ],
   "source": [
    "register_scraped_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ncode_index(cursor):\n",
    "    cursor.execute(\"CREATE INDEX ncodeindex ON details(ncode)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ポイント予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_noun_number(mecab, text):\n",
    "    text = str(text)\n",
    "    count = []\n",
    "    for line in mecab.parse(text).splitlines():\n",
    "        try:\n",
    "            if \"名詞\" in line.split()[-1]:\n",
    "                count.append(line)\n",
    "        except:\n",
    "            pass\n",
    "    return len(set(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(detail_df):\n",
    "    '''\n",
    "    Made features: \n",
    "        title_length: length of title\n",
    "        story_length: length of story\n",
    "        text_length: length of text\n",
    "        keyword_number: number of keywords\n",
    "        noun_proportion_in_text: number of nouns in text per text length\n",
    "    '''\n",
    "    mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "    \n",
    "    for column in ['title', 'story', 'text']:\n",
    "        detail_df[column + '_length'] = detail_df[column].apply(lambda x: len(str(x)))\n",
    "    detail_df['keyword_number'] = detail_df['keyword'].apply(lambda x: len(str(x).split(' ')))\n",
    "    detail_df['noun_proportion_in_text'] = detail_df.text.apply(lambda x: count_noun_number(mecab, str(x)) / len(str(x)))\n",
    "    return detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_prediction(url, detail_df):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of point prediction api\n",
    "        pandas.DataFrame detail_df: dataframe containing all features of item\n",
    "    '''\n",
    "    detail_df = preprocessing(detail_df)\n",
    "    \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {}\n",
    "    data = {column: list(detail_df[column]) for column in list(detail_df.columns)}\n",
    "    data = json.dumps(data)\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    predicted_points = r_post.json()['prediction']\n",
    "    return predicted_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_predicted_point(url, conn, cursor, mode, ncodes=None, test=True):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of point prediction api\n",
    "        sqlite3.Connection conn: connection of sqlite3\n",
    "        sqlite3.Cursor cursor: cursor of sqlite3\n",
    "        str mode: all (target all records whose predict_point is Nan) or part (specify records by ncode)\n",
    "        list<str> ncodes: specify ncode when use part mode\n",
    "    '''   \n",
    "    \n",
    "    if mode not in ['all', 'part']:\n",
    "        raise Exception('Argument mode shoud be all or part.')\n",
    "    if mode == 'part' and type(ncodes) is not list:\n",
    "        raise Exception('Argument ncodes should be list of string.')\n",
    "    \n",
    "    if mode == 'all':\n",
    "        if test == True:\n",
    "            detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE predict_point='Nan' LIMIT 10\", con=conn)\n",
    "        else:\n",
    "            detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE predict_point='Nan'\", con=conn)\n",
    "        ncodes = list(detail_df.ncode)\n",
    "    elif mode == 'part':\n",
    "        ncodes_str = ', '.join(map(str, list([\"'{}'\".format(ncode) for ncode in ncodes])))\n",
    "        detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE ncode IN ({})\".format(ncodes_str), con=conn)\n",
    "\n",
    "    predicted_points = point_prediction(url, detail_df)\n",
    "    \n",
    "    ncodes = list(detail_df.ncode)\n",
    "    for ncode, predicted_point in zip(ncodes, predicted_points):\n",
    "        c.execute(\"UPDATE details SET predict_point={} WHERE ncode='{}'\".format(predicted_point, ncode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'http://localhost:5000/predict'\n",
    "url = 'http://localhost:3033/predict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2292ed06103e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregister_predicted_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-cc6e4388563b>\u001b[0m in \u001b[0;36mregister_predicted_point\u001b[0;34m(url, conn, cursor, mode, ncodes, test)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mdetail_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SELECT * FROM details WHERE ncode IN ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncodes_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mpredicted_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetail_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mncodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-2819e244dd2b>\u001b[0m in \u001b[0;36mpoint_prediction\u001b[0;34m(url, detail_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mr_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mpredicted_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "register_predicted_point(url, conn, cursor, mode='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量登録"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of feature extraction api\n",
    "        list<str> texts: texts of narou novel\n",
    "    Return:\n",
    "        list<float> features: feature vectors of item\n",
    "    '''   \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {'texts': texts}\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "    features = r_post.json()['prediction']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(ncodes, features):\n",
    "    for ncode, feature in zip(ncodes, features):\n",
    "        yield {\n",
    "            '_index': 'features',\n",
    "            'ncode': ncode,\n",
    "            'feature': feature\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_features_to_elasticsearch(host, url, ncodes, texts, h_dim=64):\n",
    "    '''\n",
    "    Args: \n",
    "        str host: host name of elasticsearch\n",
    "        str url: url of feature extraction api\n",
    "        list<str> ncodes: ncodes to register\n",
    "        texts<str> texts: texts to extract features\n",
    "        h_dim: size of feature vector\n",
    "    '''    \n",
    "\n",
    "    features = extract_features(url, texts)\n",
    "    \n",
    "    client = Elasticsearch(host)\n",
    "    \n",
    "    mappings = {\n",
    "        'properties': {\n",
    "            'ncode': {'type': 'text'},\n",
    "            'feature': {'type': 'dense_vector', 'dims': h_dim}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not client.indices.exists(index='features'):\n",
    "        client.indices.create(index='features', body={ 'mappings': mappings })\n",
    "    \n",
    "    bulk(client, generate_data(ncodes, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_all_features_to_elasticsearch(conn, host, url, h_dim=64, test=True):\n",
    "    \n",
    "    if test == True:\n",
    "        detail_df = pd.read_sql_query(sql=\"SELECT * FROM details LIMIT 30\", con=conn)\n",
    "    else:\n",
    "        detail_df = pd.read_sql_query(sql=\"SELECT * FROM details WHERE predict_point=1\", con=conn)\n",
    "    \n",
    "    ncodes = list(detail_df.ncode)\n",
    "    texts = list(detail_df.text)\n",
    "    \n",
    "    register_features_to_elasticsearch(host, url, ncodes, texts, h_dim=h_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'localhost:9200'\n",
    "# url = 'http://localhost:5000/predict'\n",
    "url = 'http://localhost:3032/predict'\n",
    "h_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "register_all_features_to_elasticsearch(conn, host, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 類似文書検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "ELASTICSEARCH_HOST_NAME = 'localhost:9200'\n",
    "FEATURE_EXTRACTION_URL = 'http://localhost:3032/predict'\n",
    "SCRAPING_TEXT_URL = 'http://localhost:3034/scraping_texts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarTextSearch(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = Elasticsearch(ELASTICSEARCH_HOST_NAME)\n",
    "        self.feature_prediction_url = FEATURE_EXTRACTION_URL\n",
    "    \n",
    "    \n",
    "    def similar_search_by_ncode(self, query_ncode):\n",
    "        '''\n",
    "        Args:\n",
    "            str query_ncode: ncode query for similar search\n",
    "        '''    \n",
    "        if type(query_ncode) is not str:\n",
    "            raise Exception('Argument query_ncode should be str.')\n",
    "\n",
    "        query_to_search_query_ncode = {\n",
    "            \"query\": {\n",
    "                \"term\": {\n",
    "                    \"ncode\": query_ncode\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        response = self.client.search(index='features', body=query_to_search_query_ncode)\n",
    "        \n",
    "        if len(response['hits']['hits']) == 0:\n",
    "            query_text = self._scraping_text_by_ncode(query_ncode)\n",
    "            recommend_ncodes = self.similar_search_by_text(query_text)\n",
    "            \n",
    "        else:\n",
    "            query_feature = response['hits']['hits'][0]['_source']['feature']     \n",
    "            recommend_ncodes = self._similar_search_by_feature(query_feature)\n",
    "        \n",
    "        return recommend_ncodes\n",
    "    \n",
    "    \n",
    "    def similar_search_by_text(self, query_text):\n",
    "        '''\n",
    "        Args:\n",
    "            str query_text: text query for similar search\n",
    "        '''\n",
    "        if type(query_text) is not list and type(query_text) is not str:\n",
    "            raise Exception('Argument query_text should be list or str.')\n",
    "        if type(query_text) is str:\n",
    "            query_text = [query_text]\n",
    "            \n",
    "        query_feature = self._extract_feature(query_text)[0]\n",
    "        \n",
    "        recommend_ncodes = self._similar_search_by_feature(query_feature)\n",
    "        return recommend_ncodes\n",
    "            \n",
    "        \n",
    "    def _scraping_text_by_ncode(self, ncode):\n",
    "        if type(ncode) is str:\n",
    "            ncode = [ncode]\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        data = {'ncodes': ncode}\n",
    "        r_post = requests.post(SCRAPING_TEXT_URL, headers=headers, json=data)\n",
    "        text = r_post.json()['texts']\n",
    "        return text\n",
    "        \n",
    "    \n",
    "    def _similar_search_by_feature(self, query_feature):\n",
    "        query_for_similar_search = {\n",
    "            \"query\": {\n",
    "                \"script_score\": {\n",
    "                    \"query\": {\n",
    "                        \"match_all\": {}\n",
    "                    },\n",
    "                    \"script\": {\n",
    "                        \"source\": \"cosineSimilarity(params.query_vec, doc['feature']) + 1.0\", # Elasticsearch does not allow negative scores\n",
    "                        \"params\": {\n",
    "                            \"query_vec\": query_feature\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.client.search(index='features', body=query_for_similar_search)\n",
    "        recommend_ncodes = []\n",
    "        for i in range(min(10, len(response['hits']['hits']))):\n",
    "            ncode = response['hits']['hits'][i]['_source']['ncode']\n",
    "            recommend_ncodes.append(ncode)\n",
    "        return recommend_ncodes\n",
    "        \n",
    "        \n",
    "    def _extract_feature(self, text):\n",
    "        '''\n",
    "        Args:\n",
    "            list<str> texts: texts of narou novel\n",
    "        Return:\n",
    "            list<float> features: feature vectors of item\n",
    "        '''   \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        data = {'texts': text}\n",
    "        r_post = requests.post(FEATURE_EXTRACTION_URL, headers=headers, json=data)\n",
    "        feature = r_post.json()['prediction']\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_text_search = SimilarTextSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = similar_text_search.similar_search_by_text('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = similar_text_search.similar_search_by_ncode('n6755gk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(url, texts):\n",
    "    '''\n",
    "    Args:\n",
    "        str url: url of feature extraction api\n",
    "        list<str> texts: texts of narou novel\n",
    "    Return:\n",
    "        list<float> features: feature vectors of item\n",
    "    '''   \n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {'texts': texts}\n",
    "    r_post = requests.post(url, headers=headers, json=data)\n",
    "    features = r_post.json()['prediction']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_text(query_text):\n",
    "    '''\n",
    "    Args:\n",
    "        str query_text: query of similar text search\n",
    "    '''\n",
    "    if type(query_text) is not list and type(query_text) is not str:\n",
    "        raise Exception('query_text should be list or str.')\n",
    "    if type(query_text) is str:\n",
    "        query_text = [query_text]\n",
    "        \n",
    "    es = Elasticsearch(ELASTICSEARCH_HOST_NAME)\n",
    "        \n",
    "    query_feature = extract_features(FEATURE_EXTRACTION_URL, query_text)[0]\n",
    "    \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\n",
    "                    \"match_all\": {}\n",
    "                },\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vec, doc['feature']) + 1.0\", # Elasticsearch does not allow negative scores\n",
    "                    \"params\": {\n",
    "                        \"query_vec\": query_feature\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    res = es.search(index='features', body=query)\n",
    "\n",
    "    es.close()\n",
    "    \n",
    "    recommend_ncodes = []\n",
    "    for i in range(10):\n",
    "        ncode = res['hits']['hits'][i]['_source']['ncode']\n",
    "        recommend_ncodes.append(ncode)\n",
    "    \n",
    "    return recommend_ncodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N8272FH',\n",
       " 'N7564EL',\n",
       " 'N2714GK',\n",
       " 'N5419GH',\n",
       " 'N0332GL',\n",
       " 'N9853GH',\n",
       " 'N8028FI',\n",
       " 'N6695GL',\n",
       " 'N3778GK',\n",
       " 'N5276DX']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_similar_text('N13', 'testtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(ELASTICSEARCH_HOST_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"term\": {\n",
    "#             \"_id\": \"wvpuJHQBdWJ8_819o-dq\",\n",
    "            \"ncode\": \"N6257GLa\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# query = {\n",
    "#     \"query\": {\n",
    "#         \"match_all\": {}\n",
    "#     },\n",
    "#     \"_source\" : \"ncode\"\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "res = es.search(index='features', body=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N6257GL'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['hits']['hits'][0]['_source']['ncode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Content-Type': 'application/json'}\n",
    "data = {'ncodes': ['N6257GL']}\n",
    "# data = json.dumps(data)\n",
    "r_post = requests.post('http://localhost:5000/scraping_texts', headers=headers, json=data)\n",
    "r_post.json()['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\u3000この文章はカクヨムのhttps://kakuyomu.jp/users/mizukiyuu1238から、本人がコピペしています。\\n\\n\\n\\n\\u3000まとめる事が苦手なので、その事に付《つ》いては最初《さいしょ》に謝《あやま》ります。\\n\\n\\u3000そしてこのページは嫌《いや》がらせ目的《もくてき》で書いたものでは有りません。\\n\\n\\u3000その為《ため》カクヨムの利用規約第5条(利用の制限)、3.スパム行為や嫌がらせ行為など、本規約第14条（禁止事項）で定める禁止行為があった場合には当たらないと思います。\\n\\n\\u3000もしこれが嫌《いや》がらせだとお思いの方が居《い》らっしゃいましたら、通報《つうほう》する前に、きちんとした理由を私《わたし》に連絡《れんらく》頂《いただ》けると幸いと存《ぞん》じます。\\n\\n\\n\\n\\u3000まず此方《こちら》の方を紹介《しょうかい》致《いた》します。\\n\\n\\u3000https://kakuyomu.jp/users/3939456\\n\\n\\u3000ぴ～とるいじ@3939456\\n\\n\\n\\n\\u3000これから書く事は、この方を嫌《いや》がらせや誹謗中傷《ひぼうちゅうしょう》するものではなく、私《わたし》が受けた誹謗中傷《ひぼうちゅうしょう》を説明《せつめい》する為《ため》のモノです。\\n\\n\\n\\n\\u3000まずこの方は\\n\\n\\u3000https://kakuyomu.jp/works/1177354054884638005\\n\\n\\u3000の小説《しょうせつ》を書いた上でそのフリーゲームを作り、絵が描けないので立ち絵配布(はいふ)(何故《なぜ》か東方二次創作関連の)サイトに行き、フリー素材の立ち絵をDLして(普通《ふつう》オリジナルの小説のフリーゲームに、敢《あ》えて関係《かんけい》無《な》い2次創作のキャラを使おうとは普通《ふつう》の思わない)ゲームクリアのオマケキャラとして、漫画《まんが》家たにたけしのキャラクター、罪袋《つみぶくろ》を使用。\\n\\n\\u3000もちろん東方Projectの二次創作《にじそうさく》では無《な》いし、そのフリーゲームを投稿《とうこう》されているゲーム投稿《とうこう》サイトには、二次創作《にじそうさく》キャラを使っている事の明記は無《な》く、ゲームの内容《ないよう》の説明《せつめい》画像《がぞう》には、ゲームの内容《ないよう》とは関係《かんけい》無《な》い罪袋《つみぶくろ》の映《うつ》た画像《がぞう》が表示されています。\\n\\n\\u3000普通《ふつう》誤解《ごかい》を招《まね》くから、東方Projectの二次創作《にじそうさく》だと誤解《ごかい》させてDLさせようしていない限《かぎ》り、東方Projectの二次創作《にじそうさく》キャラ、罪袋《つみぶくろ》の画像がゲームの紹介《しょうかい》画面に出て来たら、その画像を変えるか説明《せつめい》欄《らん》に説明《せつめい》をするのが普通《ふつう》の大人のする事なのですが……\\n\\n\\n\\n\\u3000誤解《ごかい》して欲《ほ》しくは無《な》いがコレに付《つ》いて私《わたし》は、ぴ～とるいじ@3939456さんが悪いと言って居《い》る訳《わけ》では有りません。\\n\\n\\u3000権利《けんり》の無《な》い私《わたし》がそんな事言ったら、それこそ犯罪《はんざい》ですから当然《とうぜん》ですね。\\n\\n\\n\\n\\u3000まぁ一次創作に関係《かんけい》無《な》い他人のキャラを、他人が描いたフリー素材で本編《ほんぺん》では無《な》いとは言え、何の説明《せつめい》も無《な》く勝手に誤解《ごかい》を招《まね》きそうな使い方で使用するべきじゃ無《な》いし、他人のキャラクター(見た目)を、他人のキャラクターだと説明《せつめい》しないで一次創作《いちじそうさく》に勝手に使うのは犯罪《はんざい》なんだけど、私《わたし》はその事を如何《どう》こう言うつもりはありません。\\n\\n\\u3000二次創作《にじそうさく》キャラの使用が問題では無《な》く、説明《せつめい》しないでの使用が問題だと、普通《ふつう》仕事をする年の大人なら分かってる筈《はず》ですし、何でゲーム投稿《とうこう》サイトのゲーム説明《せつめい》画面変えないのか、とても不思議《ふしぎ》ですが……\\n\\n\\n\\n\\u3000再度《さいど》言いますが、東方Projectの二次創作《にじそうさく》では無《な》く自分のオリジナル小説《しょうせつ》のフリーゲームに、何の説明《せつめい》も無《な》く勝手に二次創作《にじそうさく》キャラを使ってる話しです。\\n\\n\\u3000大事な事だからもう一度言うけど、わざと二次創作《にじそうさく》のキャラクターを、何の説明《せつめい》も無《な》しに一次創作《いちじそうさく》のゲームに勝手に使うのは、犯罪《はんざい》です。\\n\\n\\u3000二次創作《にじそうさく》のキャラクターとして作られている立ち絵を、勝手に一次創作《いちじそうさく》に使うのは犯罪《はんざい》。\\n\\n\\n\\n\\u3000KADOKAWA関係《かんけい》で例《たと》えるなら、涼宮ハルヒの憂鬱《ゆううつ》やスレイヤーズの二次創作《にじそうさく》漫画《まんが》を誰《だれ》かが作ったとして、そのキャラクターが人気になり、誰《だれ》かがその立ち絵を描いたとします。\\n\\n\\u3000そのキャラクターの立ち絵を、全く関係《かんけい》無《な》い自分のフリーゲームに勝手に使ったら、誰《だれ》でもそんな奴《やつ》馬鹿《ばか》やアホと言うでしょう。\\n\\n\\u3000此奴《こいつ》はそれをやってるんです。\\n\\n\\n\\n\\u3000私《わたし》はたにたけし氏じゃ無《な》いから、自分の問題に対してにしか此奴《こいつ》の問題は引き合いに出しませんけど……\\n\\n\\n\\n\\u3000此所《ここ》までが相手が如何《どう》言う人間かの説明《せつめい》。\\n\\n\\u3000私《わたし》のツイッターのツリーの方にも画像《がぞう》載《の》せていますが、\\n\\n\\u3000https://twitter.com/moonofwater2/status/1280133889453785089\\n\\n\\n\\n\\u3000ガイドラインに書いてないからと、良《い》い大人が二次創作《にじそうさく》キャラを、一次創作《いちじそうさく》に無断《むだん》や説明《せつめい》無《な》しに使うのは、犯罪《はんざい》以前に、倫理《りんり》観《かん》を疑《うたが》われるのは当たり前で、しかも元の小説《しょうせつ》(説明《せつめい》欄《らん》にそのゲーム投稿《とうこう》サイトの、自分のゲームのページ貼《は》ってる)を、\\n\\n\\n\\nhttps://kakuyomu.jp/contests/kakuyomu2020summer/detail\\n\\n\\n\\nに応募《おうぼ》する始末(記憶《きおく》違いならごめんなさい)。\\n\\n\\n\\n\\u3000そんな人間が他サイトで、そのフリーゲームの記事を頼《たの》んでいたので三次創作《さんじそうさく》ですよね(東方Projectの二次創作《にじそうさく》キャラクターを敢《あ》えて使っている為《ため》、そうでなければただの犯罪《はんざい》でしかないし)とコメントしたら、お前は既存《きぞん》の作品の大事な設定《せってい》パクって、カクヨムの規約《きやく》違反《いはん》してんじゃねーかと大嘘《おおうそ》つかれ、多分それが原因《げんいん》でそれを信じた人(VPは増えたが、きちんと読んだ形跡《けいせき》は私《わたし》の知る限《かぎ》り無《な》い)に通報《つうほう》され公開停止(ていし)された上、そのサイト(アフェリエイトサイトの為《ため》、此所《ここ》では名前を出せませんが)事態《じたい》が、上記の様な事を分かっていながら犯罪《はんざい》じゃねーから問題ねーと記事を書いた上で、私《わたし》のアドレスからの書き込みを禁止《きんし》してしまい、説明《せつめい》の機会《きかい》を奪《うば》われてしまった為《ため》、この様な説明《せつめい》を書いております。\\n\\n\\n\\n\\u3000では此処《ここ》からは私《わたし》の方の説明《せつめい》をします。\\n\\n\\n\\n\\u3000元々《もともと》この小説《しょうせつ》は、10年ぶりぐらいに小説《しょうせつ》を1年間書き溜めてみようと思って書いていたもので、公開して1年してからぴ～とるいじ@3939456の上記のフリーゲームに文句《もんく》を言ったら、何故《なぜ》かその後に公開停止(ていし)になり、再度《さいど》投稿《とうこう》し直した物です。\\n\\n\\n\\n\\u3000次の項《ページ》の説明《せつめい》にありますが、玩具《がんぐ》アニメのイメージで書いており、現実世界では有るが異世界《いせかい》でもあると言う事を表す為《ため》、神様の名前が外国語(アイヌ語含(ふく)むと)+日本語の漢字表記となっており、太陽神天照(あまてらす)の良《い》い名前が思い付《つ》かづ、子供《こども》が主人公の事も有り光堵覧《エルドラン》と仮名《かめい》を付《つ》けて、公開しました。\\n\\n\\u3000因《ちな》みにエルドランとはサンライズと言う会社の作ったアニメに出て来る光《ひか》るおじさん及《およ》び、スクエアエニックスのクルーズチェイサーブラスティーと言うゲームのラスボスの名前であり、固有《こゆう》名詞では無《な》く、言葉の響《ひび》きだけ使っても権利《けんり》侵害《しんがい》にはなりません。\\n\\n\\u3000因《ちな》みにサンライズアニメのエルドランは、小学校や町を勝手に基地《きち》に改造《かいぞう》したり、子供《こども》達《たち》にロボットを与《あた》えたりしますが、私《わたし》の小説《しょうせつ》に出て来た光堵覧《エルドラン》(今は魂輝守《アマテラス》)\\n\\nは上記の様な事は全くしません。\\n\\n\\u3000その他にもキャラが多い事も有り、キャラのイメージを本編《ほんぺん》以外《いがい》のキャラ紹介《しょうかい》で、こんなアニメのキャラのイメージでと説明《せつめい》したり(カクヨムの規約《きやく》や法律《ほうりつ》的《てき》にはOK)、声優《せいゆう》の名前やアニメキャラ名の流用(固有《こゆう》名詞は使わずカクヨムの規約《きやく》や法律《ほうりつ》的《てき》にはOKなレベルで)でイメージをし易《やす》い様にしたりしていました。\\n\\n\\u3000例《たと》えば小学生(祖父《そふ》が故人《こじん》の大学教授(きょうじゅ))女子(彼氏《かれし》の家が修理《しゅうり》工《こう》)が、テレビで特撮《とくさつ》のジュウレンジャーやっていた事で恐竜《キョウリュウ》型《がた》合体ロボットに、マキナ(機械《きかい》)ザウラー(恐竜《きょうりゅう》)と付《つ》けていたり、海蛇名方《タケミナカタ》(小説《しょうせつ》の話し内の諏訪《すわ》(長野)の風神)が、考えあって雷獣《らいじゅう》から作ったロボットと別のロボットを戦わせ、その雷獣《らいじゅう》のロボットの名前が、冠身輝鎚《たけみかづち》(小説《しょうせつ》の話し内の剣《けん》と雷《かみなり》の神)の名前を模《も》して雷獣王《らいじゅうおう》(雷《らい》=雷神/獣《じゅう》=雷《かみなり》は赤《せき》龍《りゅう》=獣《けもの》/王=何か凄《すご》い=神)とかだったり。\\n\\n\\u3000勿論《もちろん》問題が起きないであろう、法律《ほうりつ》や規約《きやく》に反しない範囲《はんい》で、こんなロボットですと言う説明《せつめい》を、本編《ほんぺん》とは別《べつ》の所でやった上で。\\n\\n\\n\\n\\u3000とは言え話し自体はオリジナルであり、名称《めいしょう》や名前の使用方法もきちんとした文句《もんく》なら、申し訳《わけ》有りませんと謝《あやま》る気持ちは有りました。\\n\\n\\u3000と言うか、何所《どこ》等辺(へん)までカクヨムでは許《ゆる》されるのか調べる為《ため》にわざとやっていたのですが……\\n\\n\\n\\n\\u3000と、此所《ここ》までが私《わたし》の……と言うか、元々《もともと》のこの小説《しょうせつ》に付《つ》いての事なのですが、再度《さいど》ぴ～とるいじ@3939456の話しに戻《もど》します。\\n\\n\\n\\n\\u3000私は2019年の4月に、自分の小説《しょうせつ》を上記で説明《せつめい》したサイトに晒《さら》しました。\\n\\n\\u3000もちろん権版《けんばん》の名前と同じ響《ひび》きのキャラで興味《きょうみ》を引こう\\n\\n\\u3000等とはしていません。\\n\\n\\u3000名前に付《つ》いて違法《いほう》では無《な》いと説明《せつめい》しようとしたら、管理《かんり》者がメールで炎上の可能性《かのうせい》が有るから、それは書くなと言われたぐらいです。\\n\\n\\n\\n\\u3000そしてエルドランシリーズのイメージですと説明《せつめい》した上で晒《さら》しがスタートしました。\\n\\n\\u3000案の定、お前のは二次創作《にじそうさく》だとかなり頓珍漢《とんちんかん》な意見を頂《いただ》きました。\\n\\n\\u3000因《ちな》みに二次創作《にじそうさく》とは、原作の舞台《ぶたい》や登場人物を使用して初めて二次創作《にじそうさく》と言います。\\n\\n\\u3000だから私《わたし》は二次創作《にじそうさく》では有りませんよと、その方にコメントを返したのですが、その時にぴ～とるいじ@3939456が横から書き込んで来たのが、二次創作《にじそうさく》じゃ無《な》いけど、お前のは既存《きぞん》の設定《せってい》の大事な部分を使っているからカクヨムでは違反《いはん》だと言う言葉。\\n\\n\\u3000そう、自分は他人の二次創作《にじそうさく》のキャラを勝手に使用しておいて、わざとそんな嘘《うそ》を私《わたし》に言ったのです。\\n\\n\\n\\n\\u3000その時はゲームの存在《そんざい》を知らず、自身の小説《しょうせつ》も誤字《ごじ》脱字《だつじ》(と言うか地の文がすっぽり無《な》かったりしました)も多かった為《ため》、ただのマウントか何かだと思っていたのですが……\\n\\n\\n\\n\\u3000余《あま》りにも私《わたし》へのコメントがおかしかったので、暫《しばら》くしてから少し人となりが気になり、ぴ～とるいじ@3939456さんの近況ノートを覗《のぞ》いてみると、他の方からカクヨムの規約《きやく》で禁止《きんし》されている、自分の小説《しょうせつ》に誘導《ゆうどう》する自主企画(きかく)を立てて、他のユーザーに注意されその企画《きかく》を消したという、他ユーザーからのコメントが(その近況ノートはぴ～とるいじ@3939456は削除《さくじょ》済《ず》み)。\\n\\n\\n\\n\\u3000この時私(わたし)はこの方がどんな方か理解《りかい》しました。\\n\\n\\n\\n\\u3000そして2020年の5月。\\n\\n\\u3000ぴ～とるいじ@3939456さんが、二次創作《にじそうさく》のキャラを勝手に使ったフリーゲームを、上で出ているサイトで晒《さら》した為《ため》、私《わたし》自身には罪袋《つみぶくろ》(東方Projectの二次創作《にじそうさく》キャラクター)の著作《ちょさく》者人格権(けん)(正確《せいかく》には同一性(せい)保持《ほじ》権 (無断で著作物を改変されて誤解を受けない権利《けんり》))は有りませんでしたので、三次創作《さんじそうさく》ですよね(東方Projectの二次創作《にじそうさく》キャラクターを敢《あ》えて使っている為《ため》、そうでなければただの犯罪《はんざい》でしかないし)と、サイト管理《かんり》者自身が炎上する様な書き込《こ》みはするなと言ったから、犯罪《はんざい》だと言わずに、そう回りくどくその使い方は間違《まち》がってるし、下手したらネットの晒《さら》し物になると注意してるのに、お前の小説《しょうせつ》は既存《きぞん》の重大な設定《せってい》を使った、カクヨムの規約《きやく》違反《いはん》の作品だと大嘘《おおうそ》を書き込《こ》んで、わざと嫌《いや》がらせをした上で、再三の注意に同じ嘘《うそ》で返した上で、その嘘《うそ》を理由に私《わたし》の小説《しょうせつ》が公開停止(ていし)になった時には、わざわざ私《わたし》の小説《しょうせつ》が公開停止(ていし)になった事を確認《かくにん》した上で、公開停止(ていし)されたからお前の方が悪いといつの間にか、何故《なぜ》か勝手に勝負してる事にされた上で、本人が別の人間が名前を変えて書き込《こ》んだのか知りませんが、ぴ～とるいじ@3939456(普通《ふつう》に犯罪《はんざい》)より私《わたし》の方(法《ほう》に触《ふ》れないレベルの名前の流用と、既存《きぞん》キャラを使ったキャライメージの説明《せつめい》。因《ちな》みにキャラの説明《せつめい》は本編《ほんぺん》外で行っており、こんなキャライメージですと言う事自体は、法《ほう》的《てき》にも規約《きやく》的にも問題は有りません)が酷《ひど》い事に、小説《しょうせつ》が公開停止(ていし)して見え無《な》い事を良《い》い事に犯罪《はんざい》者扱(あつか)い。\\n\\n\\n\\n\\u3000流石《さすが》にこの書き込《こ》みはただの犯罪《はんざい》なので、サイト管理《かんり》者に少し相談《そうだん》したら、サイト管理《かんり》者は何故《なぜ》か犯罪《はんざい》じゃなきゃ問題無(な)いと言う記事を作った上で、私《わたし》のPCからの書き込《こ》みを禁止《きんし》に……\\n\\n\\n\\n\\u3000サイト管理《かんり》者は何であんな記事を作ったのか?\\n\\n\\u3000そして何で書き込《こ》みを禁止《きんし》にしたのか?\\n\\n\\n\\n\\u3000勝手に著作《ちょさく》者人格権(けん)侵害《しんがい》以上《いじょう》の悪い事を、私《わたし》がやったと嘘《うそ》を書き込《こ》む事は犯罪《はんざい》だし、その事に付《つ》いての本人の説明《せつめい》を、故意《こい》に出来なくする事も犯罪《はんざい》です。\\n\\n\\n\\n\\u3000とこんな事が有ったので、私《わたし》は誰《だれ》かに誤解《ごかい》されたままでは困《こま》り、著作《ちょさく》者人格権(けん)侵害《しんがい》以上《いじょう》の事をやったと思われているのが嫌《いや》なのです。\\n\\n\\n\\n\\u3000そりゃぁ誰《だれ》が上記の場面を見ていたか分からないし、皆《みな》さんだって自分が著作《ちょさく》者人格権(けん)侵害《しんがい》以上《いじょう》の事をやったと書き込《こ》まれて、そのサイト管理《かんり》者から書き込《こ》み禁止《きんし》しされたら、如何《どう》にか誤解《ごかい》を解《と》きたいしたいと思いますよね。\\n\\n\\u3000そりゃ、私《わたし》の事を犯罪《はんざい》者と思い込《こ》んでる人だって、居《い》るかもしれないんだから。\\n\\n\\u3000元々《もともと》は普通《ふつう》に、小説《しょうせつ》を再度《さいど》投稿《とうこう》しようと思っていましたが、上記の様な出来事が有った為《ため》、問題が起こらない様にこの文章を入れる事を決めました。\\n\\n\\n\\n\\u3000以下《いか》の状況《じょうきょう》で、元の小説《しょうせつ》が公開停止(ていし)に成《な》っていますので、別の理由でこの小説《しょうせつ》を知った場合は、私《わたし》に対して犯罪《はんざい》者と思わない事を切《せつ》に願《ねが》います。\\n\\n\\n\\n\\n\\n\\n\\n※再度《さいど》書きますが、私《わたし》は法《ほう》を犯《おか》してはおりません。\\n\\n\\u3000もし別《べつ》の理由でこの元の小説《しょうせつ》を見て、私《わたし》が犯罪《はんざい》しゃだと誤解《ごかい》されている方が居たら、きちんとサンライズかスクエアエニックスに、上記の事を説明《せつめい》した上で、きちんと確認《かくにん》を取った上で私《わたし》に御《ご》連絡《れんらく》下さい。\\n\\n\\u3000元の小説《しょうせつ》の公開停止(ていし)理由は多分、パロディぽい所が多かった為《ため》だたと、私《わたし》は考えています。\\n\\n\\u3000まぁ何で公開して1年後のタイミングで、特定《とくてい》のユーザーにそれは普通《ふつう》の人はしないと教えたタイミングで、後悔《こうかい》停止《ていし》に成《な》ったのかは疑問《ぎもん》ですが。\\n\\n\\n\\n※問題が有りそうな名前(光堵覧《エルドラン》等)を変えて再《さい》投稿《とうこう》しているのは、カクヨムの運営《うんえい》が公開から1年して公開停止(ていし)だと決めた、原因《げんいん》を調べると共《とも》に、私《わたし》は心から他サイトで私《わたし》にぴ～とるいじ@3939456がコメントした、既存《きぞん》の作品から重要な設定《せってい》を拝借《はいしゃく》するのは、規約《きやく》違反《いはん》になると言う、重要な設定《せってい》と言うのがどんなものか調べる為《ため》です。\\n\\n画像《がぞう》では名前を隠していますが、気が代って対象者の名前を出しています。\\n\\n\\n\\n\\n\\n\\n\\n\\u3000玩具《がんぐ》アニメの登場人物に重要な設定《せってい》なんて有る筈《はず》ありませんし(固有《こゆう》名詞でもなければ、エルドランシリーズと言う名称《めいしょう》も、シリーズ終了《しゅうりょう》後に、必要《ひつよう》になって付けた物なので、重要な設定《せってい》では無いと思います)。\\n\\n']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_post.json()['texts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
